{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c0c756-b1c5-487d-8172-0999a73487cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/justinenglemann/llm/mediqa-oe/evaluation\n",
      "/home/justinenglemann/llm\n",
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (18432) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 2.4340s.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130913aa55c048f29c7e64e259eb03b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx=0 took 21.6742s curr_sample[\"id\"]='primock57_1_5' len(pred)=1\n",
      "idx=1 took 3.5893s curr_sample[\"id\"]='acibench_D2N188_aci_clef_taskC_test3' len(pred)=2\n",
      "idx=2 took 5.1326s curr_sample[\"id\"]='primock57_2_1' len(pred)=1\n",
      "idx=3 took 2.8207s curr_sample[\"id\"]='acibench_D2N079_aci_valid' len(pred)=2\n",
      "idx=4 took 3.3141s curr_sample[\"id\"]='acibench_D2N069_virtassist_valid' len(pred)=2\n",
      "idx=5 took 3.3752s curr_sample[\"id\"]='acibench_D2N036_aci_train' len(pred)=3\n",
      "idx=6 took 2.7577s curr_sample[\"id\"]='acibench_D2N202_aci_clef_taskC_test3' len(pred)=2\n",
      "idx=7 took 2.5224s curr_sample[\"id\"]='acibench_D2N172_virtassist_clef_taskC_test3' len(pred)=1\n",
      "idx=8 took 3.9195s curr_sample[\"id\"]='acibench_D2N033_aci_train' len(pred)=4\n",
      "idx=9 took 3.4064s curr_sample[\"id\"]='acibench_D2N194_aci_clef_taskC_test3' len(pred)=2\n",
      "idx=10 took 2.6786s curr_sample[\"id\"]='acibench_D2N013_virtassist_train' len(pred)=2\n",
      "idx=11 took 3.7879s curr_sample[\"id\"]='acibench_D2N071_virtassist_valid' len(pred)=2\n",
      "idx=12 took 4.7267s curr_sample[\"id\"]='acibench_D2N048_aci_train' len(pred)=3\n",
      "idx=13 took 4.0841s curr_sample[\"id\"]='primock57_5_1' len(pred)=1\n",
      "idx=14 took 4.6324s curr_sample[\"id\"]='primock57_5_6' len(pred)=1\n",
      "idx=15 took 2.7801s curr_sample[\"id\"]='acibench_D2N159_aci_clinicalnlp_taskC_test2' len(pred)=2\n",
      "idx=16 took 8.1737s curr_sample[\"id\"]='acibench_D2N004_virtassist_train' len(pred)=5\n",
      "idx=17 took 2.8308s curr_sample[\"id\"]='acibench_D2N180_virtscribe_clef_taskC_test3' len(pred)=1\n",
      "idx=18 took 4.3744s curr_sample[\"id\"]='primock57_5_2' len(pred)=1\n",
      "idx=19 took 5.2334s curr_sample[\"id\"]='acibench_D2N143_virtscribe_clinicalnlp_taskC_test2' len(pred)=6\n",
      "idx=20 took 3.0336s curr_sample[\"id\"]='acibench_D2N147_aci_clinicalnlp_taskC_test2' len(pred)=2\n",
      "idx=21 took 10.3675s curr_sample[\"id\"]='primock57_2_7' len(pred)=1\n",
      "idx=22 took 8.0894s curr_sample[\"id\"]='acibench_D2N127_aci_clinicalnlp_taskB_test1' len(pred)=5\n",
      "idx=23 took 5.2415s curr_sample[\"id\"]='acibench_D2N175_virtassist_clef_taskC_test3' len(pred)=4\n",
      "idx=24 took 3.2612s curr_sample[\"id\"]='acibench_D2N056_aci_train' len(pred)=3\n",
      "idx=25 took 7.1416s curr_sample[\"id\"]='primock57_3_9' len(pred)=1\n",
      "idx=26 took 3.6165s curr_sample[\"id\"]='acibench_D2N154_aci_clinicalnlp_taskC_test2' len(pred)=3\n",
      "idx=27 took 6.4408s curr_sample[\"id\"]='acibench_D2N201_aci_clef_taskC_test3' len(pred)=3\n",
      "idx=28 took 2.7361s curr_sample[\"id\"]='acibench_D2N084_aci_valid' len(pred)=2\n",
      "idx=29 took 2.8604s curr_sample[\"id\"]='acibench_D2N179_virtscribe_clef_taskC_test3' len(pred)=1\n",
      "idx=30 took 7.6021s curr_sample[\"id\"]='acibench_D2N138_virtscribe_clinicalnlp_taskC_test2' len(pred)=4\n",
      "idx=31 took 3.2785s curr_sample[\"id\"]='acibench_D2N045_aci_train' len(pred)=3\n",
      "idx=32 took 5.6843s curr_sample[\"id\"]='acibench_D2N195_aci_clef_taskC_test3' len(pred)=5\n",
      "idx=33 took 6.1831s curr_sample[\"id\"]='primock57_2_3' len(pred)=1\n",
      "idx=34 took 4.7527s curr_sample[\"id\"]='acibench_D2N078_aci_valid' len(pred)=3\n",
      "idx=35 took 2.9718s curr_sample[\"id\"]='acibench_D2N137_virtassist_clinicalnlp_taskC_test2' len(pred)=1\n",
      "idx=36 took 6.4697s curr_sample[\"id\"]='primock57_1_9' len(pred)=1\n",
      "Requested tokens (19272) exceed context window of 18432\n",
      "idx=37 took 0.0163s curr_sample[\"id\"]='primock57_1_11' len(pred)=1\n",
      "idx=38 took 6.0378s curr_sample[\"id\"]='primock57_5_3' len(pred)=2\n",
      "idx=39 took 2.6718s curr_sample[\"id\"]='acibench_D2N116_aci_clinicalnlp_taskB_test1' len(pred)=2\n",
      "idx=40 took 3.6880s curr_sample[\"id\"]='acibench_D2N184_virtscribe_clef_taskC_test3' len(pred)=2\n",
      "idx=41 took 5.7667s curr_sample[\"id\"]='acibench_D2N117_aci_clinicalnlp_taskB_test1' len(pred)=7\n",
      "idx=42 took 5.8046s curr_sample[\"id\"]='acibench_D2N087_aci_valid' len(pred)=6\n",
      "idx=43 took 6.4391s curr_sample[\"id\"]='acibench_D2N058_aci_train' len(pred)=9\n",
      "idx=44 took 5.2403s curr_sample[\"id\"]='acibench_D2N003_virtassist_train' len(pred)=3\n",
      "idx=45 took 6.5353s curr_sample[\"id\"]='acibench_D2N163_aci_clinicalnlp_taskC_test2' len(pred)=8\n",
      "idx=46 took 2.8741s curr_sample[\"id\"]='acibench_D2N100_virtscribe_clinicalnlp_taskB_test1' len(pred)=1\n",
      "idx=47 took 2.9149s curr_sample[\"id\"]='acibench_D2N106_aci_clinicalnlp_taskB_test1' len(pred)=2\n",
      "idx=48 took 4.2632s curr_sample[\"id\"]='acibench_D2N141_virtscribe_clinicalnlp_taskC_test2' len(pred)=1\n",
      "idx=49 took 6.6195s curr_sample[\"id\"]='acibench_D2N139_virtscribe_clinicalnlp_taskC_test2' len(pred)=3\n",
      "idx=50 took 4.5487s curr_sample[\"id\"]='acibench_D2N029_virtscribe_train' len(pred)=2\n",
      "idx=51 took 4.2845s curr_sample[\"id\"]='acibench_D2N052_aci_train' len(pred)=5\n",
      "idx=52 took 8.0578s curr_sample[\"id\"]='acibench_D2N192_aci_clef_taskC_test3' len(pred)=7\n",
      "idx=53 took 3.4404s curr_sample[\"id\"]='acibench_D2N171_virtassist_clef_taskC_test3' len(pred)=2\n",
      "idx=54 took 6.8045s curr_sample[\"id\"]='acibench_D2N134_virtassist_clinicalnlp_taskC_test2' len(pred)=4\n",
      "idx=55 took 3.5115s curr_sample[\"id\"]='acibench_D2N097_virtassist_clinicalnlp_taskB_test1' len(pred)=2\n",
      "idx=56 took 3.9109s curr_sample[\"id\"]='acibench_D2N123_aci_clinicalnlp_taskB_test1' len(pred)=5\n",
      "Requested tokens (20120) exceed context window of 18432\n",
      "idx=57 took 0.0169s curr_sample[\"id\"]='primock57_1_7' len(pred)=5\n",
      "idx=58 took 5.2210s curr_sample[\"id\"]='acibench_D2N111_aci_clinicalnlp_taskB_test1' len(pred)=6\n",
      "idx=59 took 5.8734s curr_sample[\"id\"]='acibench_D2N032_virtscribe_train' len(pred)=3\n",
      "idx=60 took 3.7933s curr_sample[\"id\"]='acibench_D2N166_aci_clinicalnlp_taskC_test2' len(pred)=2\n",
      "idx=61 took 5.0308s curr_sample[\"id\"]='acibench_D2N035_aci_train' len(pred)=5\n",
      "idx=62 took 3.2082s curr_sample[\"id\"]='acibench_D2N062_aci_train' len(pred)=3\n",
      "idx=63 took 3.5171s curr_sample[\"id\"]='acibench_D2N129_virtassist_clinicalnlp_taskC_test2' len(pred)=1\n",
      "idx=64 took 5.0665s curr_sample[\"id\"]='acibench_D2N170_virtassist_clef_taskC_test3' len(pred)=2\n",
      "idx=65 took 7.5023s curr_sample[\"id\"]='primock57_2_2' len(pred)=2\n",
      "idx=66 took 4.7646s curr_sample[\"id\"]='acibench_D2N009_virtassist_train' len(pred)=4\n",
      "idx=67 took 6.1718s curr_sample[\"id\"]='primock57_2_4' len(pred)=1\n",
      "idx=68 took 3.5989s curr_sample[\"id\"]='acibench_D2N186_aci_clef_taskC_test3' len(pred)=4\n",
      "idx=69 took 5.9815s curr_sample[\"id\"]='acibench_D2N050_aci_train' len(pred)=5\n",
      "idx=70 took 7.4649s curr_sample[\"id\"]='acibench_D2N198_aci_clef_taskC_test3' len(pred)=8\n",
      "idx=71 took 5.2431s curr_sample[\"id\"]='acibench_D2N161_aci_clinicalnlp_taskC_test2' len(pred)=6\n",
      "idx=72 took 2.6284s curr_sample[\"id\"]='acibench_D2N112_aci_clinicalnlp_taskB_test1' len(pred)=1\n",
      "idx=73 took 4.0273s curr_sample[\"id\"]='primock57_5_7' len(pred)=1\n",
      "idx=74 took 3.0177s curr_sample[\"id\"]='acibench_D2N051_aci_train' len(pred)=2\n",
      "idx=75 took 4.9401s curr_sample[\"id\"]='acibench_D2N016_virtassist_train' len(pred)=3\n",
      "idx=76 took 8.5965s curr_sample[\"id\"]='acibench_D2N073_virtscribe_valid' len(pred)=9\n",
      "idx=77 took 7.2912s curr_sample[\"id\"]='primock57_4_8' len(pred)=1\n",
      "idx=78 took 4.6626s curr_sample[\"id\"]='acibench_D2N006_virtassist_train' len(pred)=2\n",
      "idx=79 took 5.1891s curr_sample[\"id\"]='acibench_D2N181_virtscribe_clef_taskC_test3' len(pred)=2\n",
      "idx=80 took 2.9166s curr_sample[\"id\"]='acibench_D2N114_aci_clinicalnlp_taskB_test1' len(pred)=2\n",
      "idx=81 took 4.0328s curr_sample[\"id\"]='primock57_3_4' len(pred)=1\n",
      "idx=82 took 7.0267s curr_sample[\"id\"]='acibench_D2N066_aci_train' len(pred)=9\n",
      "idx=83 took 5.1813s curr_sample[\"id\"]='acibench_D2N077_aci_valid' len(pred)=4\n",
      "idx=84 took 1.8685s curr_sample[\"id\"]='acibench_D2N187_aci_clef_taskC_test3' len(pred)=1\n",
      "idx=85 took 4.2202s curr_sample[\"id\"]='acibench_D2N046_aci_train' len(pred)=5\n",
      "idx=86 took 8.7846s curr_sample[\"id\"]='primock57_1_6' len(pred)=4\n",
      "idx=87 took 2.9098s curr_sample[\"id\"]='acibench_D2N019_virtassist_train' len(pred)=1\n",
      "idx=88 took 2.8342s curr_sample[\"id\"]='acibench_D2N176_virtassist_clef_taskC_test3' len(pred)=1\n",
      "idx=89 took 2.5718s curr_sample[\"id\"]='acibench_D2N064_aci_train' len(pred)=2\n",
      "idx=90 took 2.8086s curr_sample[\"id\"]='acibench_D2N083_aci_valid' len(pred)=2\n",
      "idx=91 took 10.6374s curr_sample[\"id\"]='acibench_D2N021_virtscribe_train' len(pred)=6\n",
      "idx=92 took 2.7560s curr_sample[\"id\"]='acibench_D2N153_aci_clinicalnlp_taskC_test2' len(pred)=2\n",
      "idx=93 took 3.1382s curr_sample[\"id\"]='acibench_D2N094_virtassist_clinicalnlp_taskB_test1' len(pred)=2\n",
      "idx=94 took 3.8527s curr_sample[\"id\"]='acibench_D2N193_aci_clef_taskC_test3' len(pred)=3\n",
      "idx=95 took 6.5222s curr_sample[\"id\"]='acibench_D2N001_virtassist_train' len(pred)=4\n",
      "idx=96 took 5.9102s curr_sample[\"id\"]='primock57_2_8' len(pred)=2\n",
      "idx=97 took 6.2153s curr_sample[\"id\"]='acibench_D2N144_virtscribe_clinicalnlp_taskC_test2' len(pred)=3\n",
      "idx=98 took 3.9771s curr_sample[\"id\"]='primock57_3_5' len(pred)=1\n",
      "idx=99 took 2.8773s curr_sample[\"id\"]='acibench_D2N132_virtassist_clinicalnlp_taskC_test2' len(pred)=1\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from llama_cpp import Llama, LlamaGrammar, LLAMA_ROPE_SCALING_TYPE_YARN\n",
    "from llama_cpp.llama_speculative import LlamaPromptLookupDecoding\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "%cd mediqa-oe/evaluation\n",
    "from evaluate_oe import evaluate_sample\n",
    "evaluate_sample\n",
    "%cd ../..\n",
    "\n",
    "with open('data/orders_data_transcript.json', 'r') as f:\n",
    "    traindata = json.loads(f.read())\n",
    "\n",
    "\n",
    "with open('data/test_orders_data_transcript.json', 'r') as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "startmodelloading_time = time.time()\n",
    "\n",
    "try:\n",
    "    llm\n",
    "except NameError:\n",
    "    print('Loading model')\n",
    "    # llm = Llama(\n",
    "    #     # model_path=\"/home/justin/llama/Qwen3-8B-Q5_K_M.gguf\",  \n",
    "    #     model_path=\"/home/justin/llama/Qwen3-14B-Q4_K_M.gguf\",  \n",
    "    #     # model_path=\"/home/justin/llama/Qwen3-14B-UD-Q4_K_XL.gguf\",  \n",
    "    #     # model_path=\"/home/justin/llama/Qwen3-4B-Q6_K.gguf\",  \n",
    "    #     draft_model=LlamaPromptLookupDecoding(),\n",
    "    #     logits_all=True,\n",
    "    #     n_gpu_layers=-1,\n",
    "    #     flash_attn=True,\n",
    "    #     n_ctx=int(4096*5.7),\n",
    "    #     verbose=False,\n",
    "    #     n_threads=os.cpu_count() - 2\n",
    "    # )\n",
    "    llm = Llama(\n",
    "        model_path=\"Qwen3-32B-Q4_K_M.gguf\",\n",
    "        n_gpu_layers=-1,          # keep all layers on the 4090\n",
    "        n_ctx=int(4096*4.5),               # physical KV‑cache size\n",
    "        offload_kqv=True,\n",
    "        rope_scaling_type=LLAMA_ROPE_SCALING_TYPE_YARN,   # or simply 2\n",
    "        yarn_ext_factor=6,\n",
    "        yarn_beta_fast=32,\n",
    "        yarn_beta_slow=1,\n",
    "        yarn_attn_factor=1,\n",
    "        yarn_orig_ctx=4096,\n",
    "        flash_attn=True,\n",
    "        logits_all=True,\n",
    "        draft_model=LlamaPromptLookupDecoding(),\n",
    "        n_threads=os.cpu_count() - 4,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    print(f'Loaded model {time.time()-startmodelloading_time:.4f}s.')\n",
    "\n",
    "examplestr = \\\n",
    "f\"\"\"\n",
    "EXAMPLE1START\n",
    "Transcript:\n",
    "{traindata['train'][0]['transcript']}\n",
    "Desired output:\n",
    "{traindata['train'][0]['expected_orders']}\n",
    "EXAMPLE1END\n",
    "\n",
    "EXAMPLE2START\n",
    "Transcript:\n",
    "{traindata['train'][-1]['transcript']}\n",
    "Desired output:\n",
    "{traindata['train'][-1]['expected_orders']}\n",
    "EXAMPLE2END\n",
    "\"\"\".replace('\\'', '\"')\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are an assistant for extracting medical orders from transcripts of patient doctor conversations.\n",
    "\n",
    "Medical order extraction involves identifying and structuring various medical orders —such as medications, imaging studies, lab tests, and follow-ups— based on doctor-patient conversations. \n",
    "\n",
    "The conversation is given to you in the format of a list of dicts with turn_id, speaker (doctor or patient), and transcript for each turn.\n",
    "\n",
    "You are to return a list of dicts with these keys: order_type, description, reason, and provenance. \n",
    "\n",
    "1. Return a list with one dict for each order from the conversation. Your output will be parsed with json.loads().\n",
    "2. If there is only a single order, still return a list.\n",
    "3. There are only four allowed values for the order_type: \"medication\", \"lab\", \"followup\", \"imaging\"\n",
    "4. Provenance should be a list of ints relating to the turn ids where the order was made, including directly preceeding turns where the reason was mentioned.\n",
    "5. Quote the reason verbatim from the text.\n",
    "6. Only list *new* or repeat orders. Do not list things that the patient is to continue doing that are mentioned in passing. Do not list previous exams.\n",
    "7. The above point (6.) is very important. Orders that have a \"continue\" status (e.g. \"we will continue with xanax XXmg\") are NOT considered valid orders, since we dont need to place a specific order in EHR for instance.\n",
    "8. Lab orders are fine-grained, i.e. each test is one order.\n",
    "9. If the doctor suggests an over the counter medication (e.g. pain killers), we also count that as an order - UNLESS the patient is already taking it (remember rule 6!)\n",
    "\n",
    "Make sure your output is a list (i.e. starts and ends with square brackets) of dicts, separated by commas.\n",
    "\n",
    "Examples:\n",
    "{examplestr}\n",
    "\"\"\"\n",
    "\n",
    "data_to_use = data['test']\n",
    "all_preds = {}\n",
    "all_metrics = {}\n",
    "metrics_per_sample = {}\n",
    "failed_idxs = []\n",
    "for idx, curr_sample in enumerate(tqdm(data_to_use)):\n",
    "    curr_sample = data_to_use[idx]\n",
    "    curr_transcript = curr_sample['transcript']\n",
    "    \n",
    "    user_prompt = f\"Please process this transcript. Reply only with the valid response in the desired format.\\nTRANSCRIPT START{curr_transcript}\\nTRANSCRIPT END. Please follow all the rules and instructions carefully\"\n",
    "    prompt=f\"\"\"\n",
    "    <|im_start|>system{system_prompt}<|im_end|>\n",
    "    <|im_start|>user\\n{user_prompt} \\\\nothink<|im_end|>\n",
    "    <|im_start|>assistant\\n<think>\\n</think>\\n\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    try:\n",
    "        response = llm.create_completion(\n",
    "            prompt=prompt,\n",
    "            max_tokens=int(4096*0.5),\n",
    "            temperature=0., top_k=1, top_p=1.0,\n",
    "            stop=[]\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        failed_idxs.append(idx)\n",
    "    curr_time = time.time()-start\n",
    "    \n",
    "    pred = response['choices'][0]['text']\n",
    "    if \"</think>\" in pred:\n",
    "        thought_trace = pred.split('</think>')[0]\n",
    "        print(thought_trace)\n",
    "        pred = pred.split('</think>')[-1]\n",
    "    try:\n",
    "        try:\n",
    "            pred = json.loads(pred)\n",
    "        except:\n",
    "            pred = json.loads('['+pred+']')\n",
    "            print(f'{idx=} was fixed with adding square brackets')\n",
    "        if isinstance(pred, dict):\n",
    "            print(f'{idx=} was a dict, we made it a list')\n",
    "            pred = [pred]\n",
    "    except:\n",
    "        print(f'{idx} is not proper json.\\n{pred}')\n",
    "        pred = []\n",
    "\n",
    "    all_preds[curr_sample['id']] = pred\n",
    "    print(f'{idx=} took {curr_time:.4f}s {curr_sample[\"id\"]=} {len(pred)=}')\n",
    "    # print()\n",
    "\n",
    "print('Done')\n",
    "\n",
    "with open('data/test_qw32B_v5dot6_ex_specdec_v2c.json', 'w') as f:\n",
    "    json.dump(all_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185e5329-0866-4562-8573-ecce62bb5c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 primock57_1_11\n",
      "57 primock57_1_7\n"
     ]
    }
   ],
   "source": [
    "# fallback for conversations that don't fit into vram\n",
    "\n",
    "with open('data/test_qw14B_v5dot6_ex_specdec_v2c.json', 'r') as f:\n",
    "    preds14B = json.loads(f.read())\n",
    "\n",
    "for idx in failed_idxs:\n",
    "    sample_id = data_to_use[idx]['id']\n",
    "    print(idx, sample_id)\n",
    "    all_preds[sample_id] = preds14B[sample_id]\n",
    "    \n",
    "with open('data/test_qw32B_v5dot6_ex_specdec_v2c_fixed.json', 'w') as f:\n",
    "    json.dump(all_preds, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
